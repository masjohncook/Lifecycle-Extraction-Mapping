{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f053f-74a3-4298-9c15-1fcd643c0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "from datasets import ClassLabel, Sequence\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "device = \"cuda\"\n",
    "# model_name_or_path = \"t5-large\"\n",
    "# tokenizer_name_or_path = \"t5-large\"\n",
    "model_name_or_path = \"facebook/bart-large-mnli\"\n",
    "tokenizer_name_or_path = \"facebook/bart-large-mnli\"\n",
    "\n",
    "text_column = \"sentence\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 256\n",
    "lr = 1e-2\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f5f0a-0686-4126-adbd-ec9d691ff159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,  load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "dataset =  load_dataset('csv', data_files={'train': \"dataset/augemented_training_combine_potong.csv\",\n",
    "                                             'test': 'dataset/test_filter_data_potong.csv'})\n",
    "dataset = dataset.map(\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf4f00-5d8d-4d40-b967-f20854ccb024",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = tokenizer(targets, max_length=5, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ff757-f99c-469b-9d7d-1ecce4927f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb157911-cd63-4515-8574-a5519a179ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58972aa-6386-40aa-bc65-1077173077e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb3725-cd1a-45dd-88bd-56a3bb217608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `dataset` is your combined dataset before preprocessing\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for cross-validation performance metrics\n",
    "cv_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold+1}/{k_folds}\")\n",
    "    \n",
    "    # Split dataset into current fold's training and validation subsets\n",
    "    train_subset = dataset.select(train_idx)\n",
    "    val_subset = dataset.select(val_idx)\n",
    "    \n",
    "    # Preprocess datasets\n",
    "    # Note: You may need to adjust preprocessing to be performed here if it's not feasible to preprocess the entire dataset beforehand\n",
    "    train_dataset = train_subset.map(preprocess_function, batched=True, ...)\n",
    "    eval_dataset = val_subset.map(preprocess_function, batched=True, ...)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size)\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize or reset your model and optimizer here\n",
    "    model = ... # Model initialization or reset\n",
    "    optimizer = ... # Optimizer initialization or reset\n",
    "    \n",
    "    # Training and evaluation loop (as you've defined)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training step\n",
    "        ...\n",
    "        # Evaluation step\n",
    "        ...\n",
    "    \n",
    "    # Collect performance metrics for the current fold\n",
    "    cv_metrics.append({\n",
    "        'train_loss': train_epoch_loss.item(),\n",
    "        'eval_loss': eval_epoch_loss.item(),\n",
    "        'train_ppl': train_ppl.item(),\n",
    "        'eval_ppl': eval_ppl.item()\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
